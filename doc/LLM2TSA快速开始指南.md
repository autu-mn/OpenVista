# LLM2TSA å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ å¿«é€Ÿå®ç°ç¬¬ä¸€ä¸ªåŠŸèƒ½ï¼š**æ—¶åºæ•°æ®å¢å¼ºï¼ˆæ¨¡å¼1ï¼‰**ã€‚

---

## æ­¥éª¤1ï¼šåˆ›å»ºæ¨¡å—ç»“æ„

```bash
# åœ¨ backend ç›®å½•ä¸‹æ‰§è¡Œ
mkdir -p LLM2TSA
touch LLM2TSA/__init__.py
touch LLM2TSA/enhancer.py
touch LLM2TSA/llm_client.py
touch LLM2TSA/utils.py
```

---

## æ­¥éª¤2ï¼šå®ç°LLMå®¢æˆ·ç«¯å°è£…

åˆ›å»º `backend/LLM2TSA/llm_client.py`ï¼š

```python
"""LLMå®¢æˆ·ç«¯ç»Ÿä¸€æ¥å£"""
import os
from typing import Optional
from abc import ABC, abstractmethod

try:
    from Agent.deepseek_client import DeepSeekClient
    DEEPSEEK_AVAILABLE = True
except ImportError:
    DEEPSEEK_AVAILABLE = False


class BaseLLMClient(ABC):
    """LLMå®¢æˆ·ç«¯åŸºç±»"""
    
    @abstractmethod
    def generate(self, prompt: str, max_tokens: int = 2000) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass


class UnifiedLLMClient(BaseLLMClient):
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ï¼Œå°è£…ä¸åŒLLMå®ç°"""
    
    def __init__(self, provider: str = "deepseek"):
        self.provider = provider
        self.client = self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        if self.provider == "deepseek":
            if not DEEPSEEK_AVAILABLE:
                raise ValueError("DeepSeekå®¢æˆ·ç«¯ä¸å¯ç”¨")
            return DeepSeekClient()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.provider}")
    
    def generate(self, prompt: str, max_tokens: int = 2000) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        try:
            if self.provider == "deepseek":
                return self.client.ask(prompt, context="")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.provider}")
        except Exception as e:
            print(f"LLMç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
```

---

## æ­¥éª¤3ï¼šå®ç°æ—¶åºæ•°æ®å¢å¼ºå™¨

åˆ›å»º `backend/LLM2TSA/enhancer.py`ï¼š

```python
"""æ—¶åºæ•°æ®å¢å¼ºå™¨ - æ¨¡å¼1å®ç°"""
import json
from typing import Dict, List, Optional
from .llm_client import UnifiedLLMClient


class TimeSeriesEnhancer:
    """æ—¶åºæ•°æ®å¢å¼ºå™¨ï¼šä¸ºæ—¶åºæ•°æ®æ·»åŠ è¯­ä¹‰æè¿°"""
    
    def __init__(self, llm_provider: str = "deepseek"):
        self.llm_client = UnifiedLLMClient(provider=llm_provider)
    
    def enhance_metric(self, metric_name: str, time_series: Dict[str, float]) -> Dict:
        """
        ä¸ºå•ä¸ªæŒ‡æ ‡ç”Ÿæˆå¢å¼ºä¿¡æ¯
        
        å‚æ•°:
            metric_name: æŒ‡æ ‡åç§°ï¼Œå¦‚ "OpenRank"
            time_series: æ—¶åºæ•°æ®å­—å…¸ï¼Œå¦‚ {"2020-08": 4.76, "2020-09": 4.93}
        
        è¿”å›:
            {
                "description": "æŒ‡æ ‡æè¿°",
                "trends": [...],
                "key_points": [...],
                "semantic_features": {...}
            }
        """
        if not time_series:
            return {
                "description": "æš‚æ— æ•°æ®",
                "trends": [],
                "key_points": [],
                "semantic_features": {}
            }
        
        # 1. ç”ŸæˆæŒ‡æ ‡æè¿°
        description = self._generate_description(metric_name, time_series)
        
        # 2. è¯†åˆ«è¶‹åŠ¿
        trends = self._detect_trends(metric_name, time_series)
        
        # 3. æå–å…³é”®ç‚¹
        key_points = self._extract_key_points(metric_name, time_series)
        
        # 4. ç”Ÿæˆè¯­ä¹‰ç‰¹å¾
        semantic_features = self._extract_semantic_features(time_series)
        
        return {
            "description": description,
            "trends": trends,
            "key_points": key_points,
            "semantic_features": semantic_features
        }
    
    def _generate_description(self, metric_name: str, time_series: Dict[str, float]) -> str:
        """ç”ŸæˆæŒ‡æ ‡æè¿°"""
        dates = sorted(time_series.keys())
        first_value = time_series[dates[0]]
        last_value = time_series[dates[-1]]
        data_points = len(time_series)
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹æ—¶åºæŒ‡æ ‡ç”Ÿæˆç®€æ´çš„æè¿°ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š

æŒ‡æ ‡åç§°ï¼š{metric_name}
æ•°æ®èŒƒå›´ï¼š{dates[0]} è‡³ {dates[-1]}ï¼ˆå…±{data_points}ä¸ªæœˆï¼‰
åˆå§‹å€¼ï¼š{first_value:.2f}
æœ€æ–°å€¼ï¼š{last_value:.2f}
å˜åŒ–å¹…åº¦ï¼š{((last_value - first_value) / first_value * 100):.1f}%

è¯·ç”¨è‡ªç„¶è¯­è¨€æè¿°è¿™ä¸ªæŒ‡æ ‡çš„å«ä¹‰å’Œæ•´ä½“è¶‹åŠ¿ã€‚
"""
        
        try:
            description = self.llm_client.generate(prompt, max_tokens=200)
            return description.strip()
        except Exception as e:
            print(f"ç”Ÿæˆæè¿°å¤±è´¥: {e}")
            return f"{metric_name}æŒ‡æ ‡åæ˜ äº†é¡¹ç›®ç›¸å…³çš„å˜åŒ–è¶‹åŠ¿ã€‚"
    
    def _detect_trends(self, metric_name: str, time_series: Dict[str, float]) -> List[Dict]:
        """è¯†åˆ«è¶‹åŠ¿æ¨¡å¼"""
        dates = sorted(time_series.keys())
        values = [time_series[d] for d in dates]
        
        # ç®€å•çš„è¶‹åŠ¿æ£€æµ‹ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ï¼‰
        trends = []
        
        # è®¡ç®—æ•´ä½“è¶‹åŠ¿
        if len(values) >= 2:
            overall_change = (values[-1] - values[0]) / values[0] * 100
            if overall_change > 10:
                trend_type = "ä¸Šå‡"
            elif overall_change < -10:
                trend_type = "ä¸‹é™"
            else:
                trend_type = "ç¨³å®š"
            
            trends.append({
                "period": f"{dates[0]} to {dates[-1]}",
                "type": trend_type,
                "change_percent": round(overall_change, 1),
                "description": f"æ•´ä½“å‘ˆç°{trend_type}è¶‹åŠ¿ï¼Œå˜åŒ–å¹…åº¦{abs(overall_change):.1f}%"
            })
        
        # ä½¿ç”¨LLMç”Ÿæˆæ›´è¯¦ç»†çš„è¶‹åŠ¿åˆ†æ
        if len(values) >= 6:
            recent_data = {dates[i]: values[i] for i in range(-6, 0)}
            prompt = f"""
åŸºäºä»¥ä¸‹{metric_name}æŒ‡æ ‡çš„æœ€è¿‘6ä¸ªæœˆæ•°æ®ï¼Œè¯†åˆ«è¶‹åŠ¿æ¨¡å¼ï¼š

{json.dumps(recent_data, ensure_ascii=False, indent=2)}

è¯·è¯†åˆ«ï¼š
1. è¶‹åŠ¿ç±»å‹ï¼ˆä¸Šå‡/ä¸‹é™/æ³¢åŠ¨/å‘¨æœŸæ€§ï¼‰
2. è¶‹åŠ¿æè¿°ï¼ˆ50å­—ä»¥å†…ï¼‰

æ ¼å¼ï¼šJSON
{{
    "type": "ä¸Šå‡",
    "description": "..."
}}
"""
            try:
                llm_result = self.llm_client.generate(prompt, max_tokens=300)
                # è§£æJSONç»“æœï¼ˆç®€åŒ–å¤„ç†ï¼‰
                if "ä¸Šå‡" in llm_result or "å¢é•¿" in llm_result:
                    trends.append({
                        "period": f"{dates[-6]} to {dates[-1]}",
                        "type": "ä¸Šå‡",
                        "description": llm_result[:100]
                    })
            except:
                pass
        
        return trends
    
    def _extract_key_points(self, metric_name: str, time_series: Dict[str, float]) -> List[Dict]:
        """æå–å…³é”®æ—¶é—´ç‚¹"""
        dates = sorted(time_series.keys())
        values = [time_series[d] for d in dates]
        
        key_points = []
        
        # æ‰¾åˆ°æœ€å¤§å€¼å’Œæœ€å°å€¼
        max_idx = values.index(max(values))
        min_idx = values.index(min(values))
        
        if max_idx != min_idx:
            key_points.append({
                "date": dates[max_idx],
                "value": values[max_idx],
                "type": "å³°å€¼",
                "description": f"è¾¾åˆ°å†å²æœ€é«˜å€¼ {values[max_idx]:.2f}"
            })
            
            key_points.append({
                "date": dates[min_idx],
                "value": values[min_idx],
                "type": "è°·å€¼",
                "description": f"è¾¾åˆ°å†å²æœ€ä½å€¼ {values[min_idx]:.2f}"
            })
        
        # æœ€æ–°å€¼
        key_points.append({
            "date": dates[-1],
            "value": values[-1],
            "type": "æœ€æ–°",
            "description": f"æœ€æ–°å€¼ä¸º {values[-1]:.2f}"
        })
        
        return key_points
    
    def _extract_semantic_features(self, time_series: Dict[str, float]) -> Dict:
        """æå–è¯­ä¹‰ç‰¹å¾"""
        values = list(time_series.values())
        
        if not values:
            return {}
        
        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        mean_val = sum(values) / len(values)
        max_val = max(values)
        min_val = min(values)
        std_val = (sum((x - mean_val) ** 2 for x in values) / len(values)) ** 0.5
        
        # è®¡ç®—å˜å¼‚ç³»æ•°
        cv = std_val / mean_val if mean_val > 0 else 0
        
        # è¯­ä¹‰åŒ–ç‰¹å¾
        growth_rate = "é«˜" if (values[-1] - values[0]) / values[0] > 0.5 else "ä¸­ç­‰" if (values[-1] - values[0]) / values[0] > 0.1 else "ä½"
        stability = "é«˜" if cv < 0.2 else "ä¸­ç­‰" if cv < 0.5 else "ä½"
        
        return {
            "growth_rate": growth_rate,
            "stability": stability,
            "volatility": "é«˜" if cv > 0.5 else "ä½",
            "range": f"{min_val:.2f} - {max_val:.2f}"
        }
    
    def generate_summary(self, all_metrics: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ•´ä½“è¶‹åŠ¿æ€»ç»“"""
        # ç®€åŒ–ç‰ˆï¼šåªä½¿ç”¨å…³é”®æŒ‡æ ‡
        key_metrics = {}
        for name, data in all_metrics.items():
            if "OpenRank" in name or "æ´»è·ƒåº¦" in name or "Staræ•°" in name:
                key_metrics[name] = data
        
        if not key_metrics:
            return "æš‚æ— è¶³å¤Ÿæ•°æ®ç”Ÿæˆæ€»ç»“ã€‚"
        
        prompt = f"""
åŸºäºä»¥ä¸‹æ—¶åºæŒ‡æ ‡æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½é¡¹ç›®å‘å±•è¶‹åŠ¿æ€»ç»“ï¼ˆ200å­—ä»¥å†…ï¼‰ï¼š

{json.dumps(key_metrics, ensure_ascii=False, indent=2)[:1000]}

è¯·æ€»ç»“ï¼š
1. æ•´ä½“å‘å±•è¶‹åŠ¿
2. å…³é”®å˜åŒ–ç‚¹
3. æœªæ¥å±•æœ›
"""
        
        try:
            summary = self.llm_client.generate(prompt, max_tokens=500)
            return summary.strip()
        except Exception as e:
            print(f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {e}")
            return "é¡¹ç›®æ•°æ®æ­£åœ¨åˆ†æä¸­..."
```

---

## æ­¥éª¤4ï¼šé›†æˆåˆ°API

ä¿®æ”¹ `backend/app.py`ï¼Œæ·»åŠ å¢å¼ºæ¥å£ï¼š

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥
from LLM2TSA.enhancer import TimeSeriesEnhancer

# åˆ›å»ºå¢å¼ºå™¨å®ä¾‹
enhancer = TimeSeriesEnhancer()

# æ·»åŠ æ–°çš„è·¯ç”±
@app.route('/api/enhance/<path:repo_key>/metric/<metric_name>', methods=['GET'])
def enhance_metric(repo_key, metric_name):
    """è·å–å•ä¸ªæŒ‡æ ‡çš„å¢å¼ºä¿¡æ¯"""
    try:
        # è·å–æ—¶åºæ•°æ®
        grouped = data_service.get_grouped_timeseries(repo_key)
        
        # æŸ¥æ‰¾æŒ‡å®šæŒ‡æ ‡
        metric_data = None
        for group in grouped.get('groups', {}).values():
            if metric_name in group.get('metrics', {}):
                metric_data = group['metrics'][metric_name]
                break
        
        if not metric_data:
            return jsonify({'error': f'æœªæ‰¾åˆ°æŒ‡æ ‡: {metric_name}'}), 404
        
        # æå–åŸå§‹æ•°æ®
        raw_data = {}
        for i, date in enumerate(grouped.get('timeAxis', [])):
            value = metric_data.get('data', [])[i]
            if value is not None:
                raw_data[date] = value
        
        # ç”Ÿæˆå¢å¼ºä¿¡æ¯
        enhanced = enhancer.enhance_metric(metric_name, raw_data)
        
        return jsonify(enhanced)
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

---

## æ­¥éª¤5ï¼šå‰ç«¯å±•ç¤ºï¼ˆå¯é€‰ï¼‰

åœ¨ `frontend/src/components/TimeSeriesChart.tsx` ä¸­æ·»åŠ å¢å¼ºä¿¡æ¯å±•ç¤ºï¼š

```typescript
// æ·»åŠ çŠ¶æ€
const [enhancedInfo, setEnhancedInfo] = useState<any>(null);

// è·å–å¢å¼ºä¿¡æ¯
useEffect(() => {
  if (metricName) {
    fetch(`/api/enhance/${repoKey}/metric/${metricName}`)
      .then(res => res.json())
      .then(data => setEnhancedInfo(data));
  }
}, [metricName]);

// åœ¨ç»„ä»¶ä¸­å±•ç¤º
{enhancedInfo && (
  <div className="mt-4 p-4 bg-gray-50 rounded-lg">
    <h3 className="font-semibold mb-2">æŒ‡æ ‡è¯´æ˜</h3>
    <p className="text-sm text-gray-700">{enhancedInfo.description}</p>
    
    {enhancedInfo.trends && enhancedInfo.trends.length > 0 && (
      <div className="mt-3">
        <h4 className="font-medium mb-1">è¶‹åŠ¿åˆ†æ</h4>
        {enhancedInfo.trends.map((trend: any, idx: number) => (
          <div key={idx} className="text-sm text-gray-600">
            {trend.description}
          </div>
        ))}
      </div>
    )}
  </div>
)}
```

---

## æ­¥éª¤6ï¼šæµ‹è¯•

åˆ›å»ºæµ‹è¯•è„šæœ¬ `backend/test_enhancer.py`ï¼š

```python
"""æµ‹è¯•å¢å¼ºå™¨åŠŸèƒ½"""
from LLM2TSA.enhancer import TimeSeriesEnhancer

# æµ‹è¯•æ•°æ®
test_data = {
    "2020-08": 4.76,
    "2020-09": 4.93,
    "2020-10": 5.03,
    "2020-11": 6.62,
    "2020-12": 12.65,
    "2021-01": 11.08,
    "2021-02": 5.81,
}

# åˆ›å»ºå¢å¼ºå™¨
enhancer = TimeSeriesEnhancer()

# æµ‹è¯•å¢å¼º
result = enhancer.enhance_metric("OpenRank", test_data)

print("=" * 60)
print("å¢å¼ºç»“æœï¼š")
print("=" * 60)
print(f"æè¿°ï¼š{result['description']}")
print(f"\nè¶‹åŠ¿ï¼š")
for trend in result['trends']:
    print(f"  - {trend['description']}")
print(f"\nå…³é”®ç‚¹ï¼š")
for point in result['key_points']:
    print(f"  - {point['date']}: {point['description']}")
print(f"\nè¯­ä¹‰ç‰¹å¾ï¼š{result['semantic_features']}")
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
cd backend
python test_enhancer.py
```

---

## ä¸‹ä¸€æ­¥

1. **ä¼˜åŒ–Prompt**ï¼šæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´Promptæ¨¡æ¿
2. **æ·»åŠ ç¼“å­˜**ï¼šé¿å…é‡å¤è°ƒç”¨LLM
3. **å®ç°æ¨¡å¼2**ï¼šæ—¶åºé¢„æµ‹åŠŸèƒ½
4. **å®ç°æ¨¡å¼3**ï¼šæ™ºèƒ½ä½“åˆ†æåŠŸèƒ½

---

## å¸¸è§é—®é¢˜

### Q: LLMè°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

A: å¢å¼ºå™¨å·²ç»åŒ…å«é”™è¯¯å¤„ç†ï¼Œå¤±è´¥æ—¶ä¼šè¿”å›é»˜è®¤æè¿°ã€‚æ£€æŸ¥ï¼š
- DeepSeek API Keyæ˜¯å¦æ­£ç¡®é…ç½®
- ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
- APIè°ƒç”¨æ˜¯å¦è¶…é™

### Q: å¦‚ä½•æé«˜ç”Ÿæˆè´¨é‡ï¼Ÿ

A: 
1. ä¼˜åŒ–Promptï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡
2. ä½¿ç”¨æ›´å¥½çš„LLMæ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰
3. æ·»åŠ few-shotç¤ºä¾‹

### Q: æ€§èƒ½å¦‚ä½•ä¼˜åŒ–ï¼Ÿ

A:
1. æ·»åŠ ç¼“å­˜æœºåˆ¶ï¼ˆç›¸åŒè¾“å…¥ç¼“å­˜ç»“æœï¼‰
2. æ‰¹é‡å¤„ç†å¤šä¸ªæŒ‡æ ‡
3. å¼‚æ­¥è°ƒç”¨LLM API

---

**ç¥å¼€å‘é¡ºåˆ©ï¼** ğŸš€

